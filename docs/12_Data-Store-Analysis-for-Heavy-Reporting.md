# Data Store Analysis for Heavy Reporting

**Version**: 1.0  
**Created**: December 8, 2025  
**Status**: Analysis & Recommendations

---

## ğŸ“Š Executive Summary

**Current State**: PostgreSQL is used for both OLTP (transactions) and OLAP (analytics/reporting).

**Problem**: Heavy reporting on the same database as transactions can cause performance issues, contention, and scalability challenges.

**Recommendation**: **Add a dedicated Analytics Database** (TimescaleDB or PostgreSQL read replica) for heavy reporting workloads.

---

## ğŸ” Current Architecture Analysis

### Current Data Stores

1. **PostgreSQL** - Primary database
   - Used for: OLTP (transactions) + OLAP (analytics)
   - Schema: `clients`, `engagements`, and future tables
   - Current workload: CRUD operations

2. **Redis** - Caching
   - Session management
   - Application-level caching

3. **Document Storage** - Files
   - Azure Blob / AWS S3 / Local
   - Document storage only

### Heavy Reporting Requirements

From `docs/01_Requirements.md`:
- **Real-time dashboards** with key metrics
- **Team productivity analytics**
- **Revenue and billing insights**
- **Compliance status overview**
- **Data aggregation and complex queries**
- **Export capabilities** (PDF, Excel, CSV)
- **Interactive charts and visualizations**

---

## âš ï¸ Problems with Current Approach

### 1. **Performance Contention**

**Issue**: Running heavy analytics queries on the same database as transactions causes:
- Slow transaction processing when reports run
- Lock contention between reads and writes
- Query timeouts during peak reporting hours

**Example**:
```sql
-- Heavy report query running on OLTP database
SELECT 
    DATE_TRUNC('month', created_at) as month,
    COUNT(*) as total_clients,
    SUM(CASE WHEN status = 'active' THEN 1 ELSE 0 END) as active_clients,
    AVG(engagement_count) as avg_engagements
FROM clients c
LEFT JOIN (
    SELECT client_id, COUNT(*) as engagement_count
    FROM engagements
    GROUP BY client_id
) e ON c.id = e.client_id
WHERE created_at >= NOW() - INTERVAL '12 months'
GROUP BY month
ORDER BY month;
```

This query:
- Scans large tables
- Performs complex aggregations
- Blocks or slows down concurrent transactions

### 2. **Scalability Limitations**

- **Read Replicas**: Expensive, adds complexity
- **Connection Pooling**: Limited by OLTP workload
- **Index Overhead**: Too many indexes slow down writes

### 3. **Query Complexity**

Heavy reporting requires:
- Multi-table joins
- Aggregations (SUM, AVG, COUNT)
- Time-series analysis
- Window functions
- Complex WHERE clauses

These are expensive on OLTP databases.

---

## âœ… Recommended Solutions

### **Option 1: PostgreSQL + TimescaleDB** â­ **RECOMMENDED**

**Architecture**:
```
PostgreSQL (OLTP) â†’ ETL Pipeline â†’ TimescaleDB (OLAP)
```

**Why TimescaleDB?**
- âœ… Built on PostgreSQL (familiar SQL)
- âœ… Optimized for time-series and analytics
- âœ… Automatic data retention policies
- âœ… Continuous aggregates (materialized views)
- âœ… Excellent for compliance deadlines, revenue trends
- âœ… Can run on same infrastructure

**Implementation**:
1. Keep PostgreSQL for OLTP (transactions)
2. Add TimescaleDB extension or separate instance
3. ETL pipeline syncs data (real-time or batch)
4. Analytics service queries TimescaleDB

**Pros**:
- Same SQL dialect (PostgreSQL)
- Easy migration path
- Great for time-series analytics
- Automatic aggregation support
- Lower operational overhead

**Cons**:
- Requires ETL pipeline
- Slight data latency (if batch sync)

---

### **Option 2: PostgreSQL + Read Replica + Materialized Views**

**Architecture**:
```
PostgreSQL (OLTP Primary) â†’ Streaming Replication â†’ PostgreSQL (Read Replica)
                                                          â†“
                                              Materialized Views (Pre-aggregated)
```

**Implementation**:
1. PostgreSQL primary for OLTP
2. PostgreSQL read replica for analytics
3. Materialized views for common reports
4. Refresh strategy (real-time or scheduled)

**Pros**:
- âœ… No new database technology
- âœ… Real-time data (streaming replication)
- âœ… Pre-aggregated data (materialized views)
- âœ… Familiar PostgreSQL

**Cons**:
- âš ï¸ More complex setup
- âš ï¸ Materialized views need refresh strategy
- âš ï¸ Still PostgreSQL (not optimized for analytics)

---

### **Option 3: PostgreSQL + ClickHouse** (For Very Heavy Analytics)

**Architecture**:
```
PostgreSQL (OLTP) â†’ ETL â†’ ClickHouse (OLAP)
```

**Why ClickHouse?**
- âœ… Columnar storage (excellent for analytics)
- âœ… Extremely fast aggregations
- âœ… Handles billions of rows
- âœ… Great for ad-hoc queries

**Pros**:
- âœ… Best performance for heavy analytics
- âœ… Handles massive data volumes
- âœ… Excellent for complex aggregations

**Cons**:
- âš ï¸ Different SQL dialect
- âš ï¸ More complex setup
- âš ï¸ Overkill for small-medium datasets

---

### **Option 4: Cloud Data Warehouse** (Enterprise)

**Architecture**:
```
PostgreSQL (OLTP) â†’ ETL â†’ Cloud Data Warehouse
                              â†“
                    â€¢ Snowflake
                    â€¢ BigQuery
                    â€¢ Azure Synapse
                    â€¢ Amazon Redshift
```

**Pros**:
- âœ… Fully managed
- âœ… Excellent scalability
- âœ… Built-in BI tools
- âœ… Pay-as-you-go

**Cons**:
- âš ï¸ Higher cost
- âš ï¸ Vendor lock-in
- âš ï¸ Data egress costs
- âš ï¸ Overkill for small-medium scale

---

## ğŸ¯ Final Recommendation

### **Recommended: PostgreSQL + TimescaleDB**

**Rationale**:
1. **Fits the use case**: Time-series analytics (compliance deadlines, revenue trends)
2. **Familiar technology**: PostgreSQL-based, same SQL
3. **Good performance**: Optimized for analytics workloads
4. **Cost-effective**: Can run on same infrastructure
5. **Easy migration**: Can start with PostgreSQL read replica, migrate to TimescaleDB later

**Implementation Plan**:

#### Phase 1: Immediate (Week 2-3)
- Set up PostgreSQL read replica
- Create materialized views for common reports
- Route analytics queries to read replica

#### Phase 2: Short-term (Month 2-3)
- Evaluate TimescaleDB
- Set up TimescaleDB instance
- Migrate analytics queries to TimescaleDB
- Set up ETL pipeline (real-time or batch)

#### Phase 3: Long-term (Month 4+)
- Optimize TimescaleDB with continuous aggregates
- Add data retention policies
- Scale as needed

---

## ğŸ“ Updated Architecture

### Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Application Layer                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   FastAPI    â”‚  â”‚  Analytics   â”‚  â”‚   Workflows   â”‚ â”‚
â”‚  â”‚  (Primary)   â”‚  â”‚   Service    â”‚  â”‚   Service     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                 â”‚                 â”‚
          â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Layer                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ PostgreSQL   â”‚  â”‚ TimescaleDB  â”‚  â”‚    Redis     â”‚ â”‚
â”‚  â”‚   (OLTP)     â”‚  â”‚   (OLAP)     â”‚  â”‚   (Cache)    â”‚ â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚ â”‚
â”‚  â”‚ â€¢ clients    â”‚  â”‚ â€¢ Aggregated â”‚  â”‚ â€¢ Sessions   â”‚ â”‚
â”‚  â”‚ â€¢ engagementsâ”‚  â”‚   metrics    â”‚  â”‚ â€¢ Cache      â”‚ â”‚
â”‚  â”‚ â€¢ tasks      â”‚  â”‚ â€¢ Time-seriesâ”‚  â”‚              â”‚ â”‚
â”‚  â”‚ â€¢ billing    â”‚  â”‚   data       â”‚  â”‚              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                  â”‚                           â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€ETLâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚         (Real-time or Batch)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Updated Architecture Diagram

See updated `docs/02_Architecture.md` with Analytics Database added.

---

## ğŸ”§ Implementation Details

### ETL Pipeline Options

#### Option A: Real-time (CDC - Change Data Capture)
- **Tool**: Debezium, AWS DMS, or custom solution
- **Latency**: Near real-time (< 1 second)
- **Complexity**: Higher
- **Use Case**: Real-time dashboards

#### Option B: Batch (Scheduled)
- **Tool**: Python script, Airflow, or simple cron job
- **Latency**: 5 minutes to 1 hour
- **Complexity**: Lower
- **Use Case**: Daily reports, scheduled analytics

#### Option C: Hybrid
- Real-time for critical metrics
- Batch for historical data
- Best of both worlds

### Data Sync Strategy

```python
# Example ETL Pipeline (Python)
def sync_to_analytics_db():
    """
    Sync data from PostgreSQL to TimescaleDB
    Runs every 5 minutes or on-demand
    """
    # 1. Get changed records since last sync
    last_sync = get_last_sync_timestamp()
    changed_clients = get_changed_clients(last_sync)
    changed_engagements = get_changed_engagements(last_sync)
    
    # 2. Transform and aggregate
    aggregated_data = aggregate_data(changed_clients, changed_engagements)
    
    # 3. Upsert to TimescaleDB
    upsert_to_timescaledb(aggregated_data)
    
    # 4. Update sync timestamp
    update_last_sync_timestamp()
```

### Materialized Views / Continuous Aggregates

**Example**: Pre-aggregated revenue by month

```sql
-- TimescaleDB Continuous Aggregate
CREATE MATERIALIZED VIEW revenue_by_month
WITH (timescaledb.continuous) AS
SELECT 
    time_bucket('1 month', created_at) AS month,
    COUNT(*) as total_clients,
    SUM(revenue) as total_revenue,
    AVG(revenue) as avg_revenue
FROM clients
GROUP BY month;

-- Auto-refresh policy
SELECT add_continuous_aggregate_policy('revenue_by_month',
    start_offset => INTERVAL '3 months',
    end_offset => INTERVAL '1 hour',
    schedule_interval => INTERVAL '1 hour');
```

---

## ğŸ“Š Performance Comparison

| Metric | PostgreSQL Only | PostgreSQL + Read Replica | PostgreSQL + TimescaleDB |
|--------|----------------|---------------------------|-------------------------|
| **OLTP Performance** | âš ï¸ Degraded during reports | âœ… Good (isolated) | âœ… Good (isolated) |
| **Analytics Performance** | âš ï¸ Slow (not optimized) | âš ï¸ Medium (PostgreSQL) | âœ… Fast (optimized) |
| **Complexity** | âœ… Low | âš ï¸ Medium | âš ï¸ Medium |
| **Cost** | âœ… Low | âš ï¸ Medium (2x DB) | âš ï¸ Medium (2x DB) |
| **Time-Series Support** | âŒ No | âŒ No | âœ… Yes |
| **Auto-Aggregation** | âŒ Manual | âš ï¸ Manual (MVs) | âœ… Automatic |
| **Scalability** | âš ï¸ Limited | âš ï¸ Limited | âœ… Good |

---

## ğŸ¯ Decision Matrix

### Choose **PostgreSQL + Read Replica** if:
- âœ… Budget is tight
- âœ… Reporting is simple (not heavy)
- âœ… Can accept some performance trade-offs
- âœ… Want to stay with PostgreSQL only

### Choose **PostgreSQL + TimescaleDB** if: â­ **RECOMMENDED**
- âœ… Need time-series analytics
- âœ… Heavy reporting requirements
- âœ… Want optimized analytics performance
- âœ… Can handle ETL pipeline
- âœ… Want familiar PostgreSQL SQL

### Choose **PostgreSQL + ClickHouse** if:
- âœ… Extremely heavy analytics (billions of rows)
- âœ… Need best possible performance
- âœ… Can handle different SQL dialect
- âœ… Have dedicated analytics team

### Choose **Cloud Data Warehouse** if:
- âœ… Enterprise scale
- âœ… Want fully managed solution
- âœ… Have budget for cloud services
- âœ… Need built-in BI tools

---

## ğŸ“‹ Action Items

### Immediate (This Week)
- [ ] Review this analysis with team
- [ ] Decide on analytics database solution
- [ ] Update architecture documentation

### Short-term (Next 2 Weeks)
- [ ] Set up PostgreSQL read replica (if chosen)
- [ ] Create materialized views for common reports
- [ ] Route analytics queries to read replica
- [ ] Monitor performance improvements

### Medium-term (Next Month)
- [ ] Evaluate TimescaleDB
- [ ] Set up TimescaleDB instance
- [ ] Implement ETL pipeline
- [ ] Migrate analytics queries
- [ ] Set up continuous aggregates

### Long-term (Next Quarter)
- [ ] Optimize ETL pipeline
- [ ] Add data retention policies
- [ ] Scale as needed
- [ ] Monitor and tune performance

---

## ğŸ”— Related Documents

- [Architecture Document](./02_Architecture.md) - Updated with Analytics Database
- [Requirements Document](./01_Requirements.md) - Heavy Dashboards requirements
- [Technology Stack](./03_Technology-Stack.md) - Current technology choices

---

## ğŸ“š References

- **TimescaleDB**: https://www.timescale.com/
- **ClickHouse**: https://clickhouse.com/
- **PostgreSQL Read Replicas**: https://www.postgresql.org/docs/current/high-availability.html
- **Materialized Views**: https://www.postgresql.org/docs/current/sql-creatematerializedview.html

---

## âœ… Conclusion

**Current selection is NOT optimal for heavy reporting.**

**Recommendation**: Add **TimescaleDB** (or PostgreSQL read replica initially) as a dedicated analytics database.

This will:
- âœ… Keep PostgreSQL for fast transactions
- âœ… Provide optimized analytics performance
- âœ… Support time-series analytics (compliance deadlines, revenue trends)
- âœ… Scale independently
- âœ… Maintain familiar PostgreSQL SQL

**Next Step**: Review and approve this recommendation, then proceed with implementation plan.

